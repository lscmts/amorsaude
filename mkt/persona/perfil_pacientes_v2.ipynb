{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucasmateus/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# importando bibliotecas conexão PDGT\n",
    "import json\n",
    "import argparse\n",
    "import subprocess\n",
    "import boto3\n",
    "import time\n",
    "from pyathena import connect\n",
    "import pandas.io.sql as sqlio\n",
    "import sys\n",
    "from ydata_profiling import ProfileReport\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "import boto3.session\n",
    "from botocore import exceptions\n",
    "from scipy.stats import chi2_contingency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando bibliotecas para análise\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import math\n",
    "import statsmodels.stats.api as sms\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import unicodedata\n",
    "import psycopg2\n",
    "import dask.dataframe as dd\n",
    "from collections import defaultdict\n",
    "from scipy.stats import shapiro\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from scipy.stats import mode\n",
    "from scipy.stats import f_oneway\n",
    "from sklearn.model_selection import train_test_split\n",
    "import googlemaps\n",
    "from datetime import datetime\n",
    "from googlemaps.exceptions import ApiError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomException(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aws athena connection info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aws athena connection function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'api_key' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/lucasmateus/dbt-projects/projetos_lucas/persona_amorsaude/perfil_pacientes_v2.ipynb Cell 7\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/lucasmateus/dbt-projects/projetos_lucas/persona_amorsaude/perfil_pacientes_v2.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# SETANDO API DO MAPS\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/lucasmateus/dbt-projects/projetos_lucas/persona_amorsaude/perfil_pacientes_v2.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/lucasmateus/dbt-projects/projetos_lucas/persona_amorsaude/perfil_pacientes_v2.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m#api_key = \"token\"\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/lucasmateus/dbt-projects/projetos_lucas/persona_amorsaude/perfil_pacientes_v2.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m gmaps_client \u001b[39m=\u001b[39m googlemaps\u001b[39m.\u001b[39mClient(key\u001b[39m=\u001b[39mapi_key)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'api_key' is not defined"
     ]
    }
   ],
   "source": [
    "# SETANDO API DO MAPS\n",
    "\n",
    "#api_key = \"token\"\n",
    "\n",
    "gmaps_client = googlemaps.Client(key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_proporcoes = \"\"\"\n",
    "sql query\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proporcoes = execute_athena_query(query_proporcoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regional</th>\n",
       "      <th>qtd_pacientes_homens</th>\n",
       "      <th>qtd_pacientes_mulheres</th>\n",
       "      <th>total_pacientes</th>\n",
       "      <th>proporcao_homens</th>\n",
       "      <th>proporcao_mulheres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ES</td>\n",
       "      <td>107660</td>\n",
       "      <td>196503</td>\n",
       "      <td>7612762</td>\n",
       "      <td>0.014142</td>\n",
       "      <td>0.025812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Norte</td>\n",
       "      <td>135914</td>\n",
       "      <td>250751</td>\n",
       "      <td>7612762</td>\n",
       "      <td>0.017853</td>\n",
       "      <td>0.032938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NE2</td>\n",
       "      <td>180588</td>\n",
       "      <td>362902</td>\n",
       "      <td>7612762</td>\n",
       "      <td>0.023722</td>\n",
       "      <td>0.047670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RJ</td>\n",
       "      <td>216168</td>\n",
       "      <td>404848</td>\n",
       "      <td>7612762</td>\n",
       "      <td>0.028395</td>\n",
       "      <td>0.053180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CO</td>\n",
       "      <td>231395</td>\n",
       "      <td>428580</td>\n",
       "      <td>7612762</td>\n",
       "      <td>0.030396</td>\n",
       "      <td>0.056298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MG</td>\n",
       "      <td>268464</td>\n",
       "      <td>485084</td>\n",
       "      <td>7612762</td>\n",
       "      <td>0.035265</td>\n",
       "      <td>0.063720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NE1</td>\n",
       "      <td>293109</td>\n",
       "      <td>564595</td>\n",
       "      <td>7612762</td>\n",
       "      <td>0.038502</td>\n",
       "      <td>0.074164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sul</td>\n",
       "      <td>378680</td>\n",
       "      <td>669421</td>\n",
       "      <td>7612762</td>\n",
       "      <td>0.049743</td>\n",
       "      <td>0.087934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SP Interior</td>\n",
       "      <td>440010</td>\n",
       "      <td>748676</td>\n",
       "      <td>7612762</td>\n",
       "      <td>0.057799</td>\n",
       "      <td>0.098345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SP CAV</td>\n",
       "      <td>438208</td>\n",
       "      <td>811206</td>\n",
       "      <td>7612762</td>\n",
       "      <td>0.057562</td>\n",
       "      <td>0.106559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      regional  qtd_pacientes_homens  qtd_pacientes_mulheres  total_pacientes  \\\n",
       "0           ES                107660                  196503          7612762   \n",
       "1        Norte                135914                  250751          7612762   \n",
       "2          NE2                180588                  362902          7612762   \n",
       "3           RJ                216168                  404848          7612762   \n",
       "4           CO                231395                  428580          7612762   \n",
       "5           MG                268464                  485084          7612762   \n",
       "6          NE1                293109                  564595          7612762   \n",
       "7          Sul                378680                  669421          7612762   \n",
       "8  SP Interior                440010                  748676          7612762   \n",
       "9       SP CAV                438208                  811206          7612762   \n",
       "\n",
       "   proporcao_homens  proporcao_mulheres  \n",
       "0          0.014142            0.025812  \n",
       "1          0.017853            0.032938  \n",
       "2          0.023722            0.047670  \n",
       "3          0.028395            0.053180  \n",
       "4          0.030396            0.056298  \n",
       "5          0.035265            0.063720  \n",
       "6          0.038502            0.074164  \n",
       "7          0.049743            0.087934  \n",
       "8          0.057799            0.098345  \n",
       "9          0.057562            0.106559  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_proporcoes['proporcao_homens'] = df_proporcoes['qtd_pacientes_homens'] / df_proporcoes['total_pacientes']\n",
    "df_proporcoes['proporcao_mulheres'] = df_proporcoes['qtd_pacientes_mulheres'] / df_proporcoes['total_pacientes']\n",
    "\n",
    "df_proporcoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required Sample Size: 16587\n"
     ]
    }
   ],
   "source": [
    "#COCHRAN FORMULA FOR SAMPLE SIZE - POPULAÇÃO COMPLETA\n",
    "\n",
    "def cochran_sample_size(confidence, estimated_proportion, margin_of_error):\n",
    "    z = np.abs(stats.norm.ppf((1 - confidence) / 2))  # Calculate Z-score\n",
    "    n = (z**2 * estimated_proportion * (1 - estimated_proportion)) / margin_of_error**2\n",
    "    return n\n",
    "\n",
    "# Example values\n",
    "confidence_level = 0.99  # 99% confidence\n",
    "estimated_proportion = 0.5  # Estimated proportion (50% for maximum sample size)\n",
    "margin_of_error = 0.01  # Margin of error (1%)\n",
    "\n",
    "# Calculate sample size\n",
    "sample_size = cochran_sample_size(confidence_level, estimated_proportion, margin_of_error)\n",
    "\n",
    "print(f\"Required Sample Size: {int(sample_size)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regional</th>\n",
       "      <th>qtd_pacientes_homens</th>\n",
       "      <th>qtd_pacientes_mulheres</th>\n",
       "      <th>total_pacientes</th>\n",
       "      <th>proporcao_homens</th>\n",
       "      <th>proporcao_mulheres</th>\n",
       "      <th>tamanho_amostra</th>\n",
       "      <th>amostra_homens</th>\n",
       "      <th>amostra_mulheres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ES</td>\n",
       "      <td>107660</td>\n",
       "      <td>196503</td>\n",
       "      <td>7612762</td>\n",
       "      <td>0.014142</td>\n",
       "      <td>0.025812</td>\n",
       "      <td>16587.241503</td>\n",
       "      <td>235</td>\n",
       "      <td>429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Norte</td>\n",
       "      <td>135914</td>\n",
       "      <td>250751</td>\n",
       "      <td>7612762</td>\n",
       "      <td>0.017853</td>\n",
       "      <td>0.032938</td>\n",
       "      <td>16587.241503</td>\n",
       "      <td>297</td>\n",
       "      <td>547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NE2</td>\n",
       "      <td>180588</td>\n",
       "      <td>362902</td>\n",
       "      <td>7612762</td>\n",
       "      <td>0.023722</td>\n",
       "      <td>0.047670</td>\n",
       "      <td>16587.241503</td>\n",
       "      <td>394</td>\n",
       "      <td>791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RJ</td>\n",
       "      <td>216168</td>\n",
       "      <td>404848</td>\n",
       "      <td>7612762</td>\n",
       "      <td>0.028395</td>\n",
       "      <td>0.053180</td>\n",
       "      <td>16587.241503</td>\n",
       "      <td>472</td>\n",
       "      <td>883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CO</td>\n",
       "      <td>231395</td>\n",
       "      <td>428580</td>\n",
       "      <td>7612762</td>\n",
       "      <td>0.030396</td>\n",
       "      <td>0.056298</td>\n",
       "      <td>16587.241503</td>\n",
       "      <td>505</td>\n",
       "      <td>934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MG</td>\n",
       "      <td>268464</td>\n",
       "      <td>485084</td>\n",
       "      <td>7612762</td>\n",
       "      <td>0.035265</td>\n",
       "      <td>0.063720</td>\n",
       "      <td>16587.241503</td>\n",
       "      <td>585</td>\n",
       "      <td>1057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NE1</td>\n",
       "      <td>293109</td>\n",
       "      <td>564595</td>\n",
       "      <td>7612762</td>\n",
       "      <td>0.038502</td>\n",
       "      <td>0.074164</td>\n",
       "      <td>16587.241503</td>\n",
       "      <td>639</td>\n",
       "      <td>1231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sul</td>\n",
       "      <td>378680</td>\n",
       "      <td>669421</td>\n",
       "      <td>7612762</td>\n",
       "      <td>0.049743</td>\n",
       "      <td>0.087934</td>\n",
       "      <td>16587.241503</td>\n",
       "      <td>826</td>\n",
       "      <td>1459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SP Interior</td>\n",
       "      <td>440010</td>\n",
       "      <td>748676</td>\n",
       "      <td>7612762</td>\n",
       "      <td>0.057799</td>\n",
       "      <td>0.098345</td>\n",
       "      <td>16587.241503</td>\n",
       "      <td>959</td>\n",
       "      <td>1632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SP CAV</td>\n",
       "      <td>438208</td>\n",
       "      <td>811206</td>\n",
       "      <td>7612762</td>\n",
       "      <td>0.057562</td>\n",
       "      <td>0.106559</td>\n",
       "      <td>16587.241503</td>\n",
       "      <td>955</td>\n",
       "      <td>1768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      regional  qtd_pacientes_homens  qtd_pacientes_mulheres  total_pacientes  \\\n",
       "0           ES                107660                  196503          7612762   \n",
       "1        Norte                135914                  250751          7612762   \n",
       "2          NE2                180588                  362902          7612762   \n",
       "3           RJ                216168                  404848          7612762   \n",
       "4           CO                231395                  428580          7612762   \n",
       "5           MG                268464                  485084          7612762   \n",
       "6          NE1                293109                  564595          7612762   \n",
       "7          Sul                378680                  669421          7612762   \n",
       "8  SP Interior                440010                  748676          7612762   \n",
       "9       SP CAV                438208                  811206          7612762   \n",
       "\n",
       "   proporcao_homens  proporcao_mulheres  tamanho_amostra  amostra_homens  \\\n",
       "0          0.014142            0.025812     16587.241503             235   \n",
       "1          0.017853            0.032938     16587.241503             297   \n",
       "2          0.023722            0.047670     16587.241503             394   \n",
       "3          0.028395            0.053180     16587.241503             472   \n",
       "4          0.030396            0.056298     16587.241503             505   \n",
       "5          0.035265            0.063720     16587.241503             585   \n",
       "6          0.038502            0.074164     16587.241503             639   \n",
       "7          0.049743            0.087934     16587.241503             826   \n",
       "8          0.057799            0.098345     16587.241503             959   \n",
       "9          0.057562            0.106559     16587.241503             955   \n",
       "\n",
       "   amostra_mulheres  \n",
       "0               429  \n",
       "1               547  \n",
       "2               791  \n",
       "3               883  \n",
       "4               934  \n",
       "5              1057  \n",
       "6              1231  \n",
       "7              1459  \n",
       "8              1632  \n",
       "9              1768  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_proporcoes['tamanho_amostra'] = sample_size\n",
    "df_proporcoes['amostra_homens'] = (df_proporcoes['proporcao_homens'] * sample_size).apply(lambda x: math.ceil(x))\n",
    "df_proporcoes['amostra_mulheres'] = (df_proporcoes['proporcao_mulheres'] * sample_size).apply(lambda x: math.ceil(x))\n",
    "\n",
    "df_proporcoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amostra homens SP CAV: 955\n",
      "Amostra homens SP Int: 959\n",
      "Amostra homens Sul: 826\n",
      "Amostra homens NE1: 639\n",
      "Amostra homens MG: 585\n",
      "Amostra homens CO: 505\n",
      "Amostra homens RJ: 472\n",
      "Amostra homens NE2: 394\n",
      "Amostra homens Norte: 297\n",
      "Amostra homens ES: 235\n"
     ]
    }
   ],
   "source": [
    "#TRANSFORMANDO VALORES DA TABELA EM VARIÁVEIS - HOMENS\n",
    "\n",
    "spcav_h = df_proporcoes.loc[df_proporcoes['regional'] == 'SP CAV', 'amostra_homens'].values[0]\n",
    "spint_h = df_proporcoes.loc[df_proporcoes['regional'] == 'SP Interior', 'amostra_homens'].values[0]\n",
    "sul_h = df_proporcoes.loc[df_proporcoes['regional'] == 'Sul', 'amostra_homens'].values[0]\n",
    "ne1_h = df_proporcoes.loc[df_proporcoes['regional'] == 'NE1', 'amostra_homens'].values[0]\n",
    "mg_h = df_proporcoes.loc[df_proporcoes['regional'] == 'MG', 'amostra_homens'].values[0]\n",
    "co_h = df_proporcoes.loc[df_proporcoes['regional'] == 'CO', 'amostra_homens'].values[0]\n",
    "rj_h = df_proporcoes.loc[df_proporcoes['regional'] == 'RJ', 'amostra_homens'].values[0]\n",
    "ne2_h = df_proporcoes.loc[df_proporcoes['regional'] == 'NE2', 'amostra_homens'].values[0]\n",
    "norte_h = df_proporcoes.loc[df_proporcoes['regional'] == 'Norte', 'amostra_homens'].values[0]\n",
    "es_h = df_proporcoes.loc[df_proporcoes['regional'] == 'ES', 'amostra_homens'].values[0]\n",
    "\n",
    "print(f'Amostra homens SP CAV: {spcav_h}')\n",
    "print(f'Amostra homens SP Int: {spint_h}')\n",
    "print(f'Amostra homens Sul: {sul_h}')\n",
    "print(f'Amostra homens NE1: {ne1_h}')\n",
    "print(f'Amostra homens MG: {mg_h}')\n",
    "print(f'Amostra homens CO: {co_h}')\n",
    "print(f'Amostra homens RJ: {rj_h}')\n",
    "print(f'Amostra homens NE2: {ne2_h}')\n",
    "print(f'Amostra homens Norte: {norte_h}')\n",
    "print(f'Amostra homens ES: {es_h}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amostra mulheres SP CAV: 1768\n",
      "Amostra mulheres SP Int: 1632\n",
      "Amostra mulheres Sul: 1459\n",
      "Amostra mulheres NE1: 1231\n",
      "Amostra mulheres MG: 1057\n",
      "Amostra mulheres CO: 934\n",
      "Amostra mulheres RJ: 883\n",
      "Amostra mulheres NE2: 791\n",
      "Amostra mulheres Norte: 547\n",
      "Amostra hommulheresens ES: 429\n"
     ]
    }
   ],
   "source": [
    "#TRANSFORMANDO VALORES DA TABELA EM VARIÁVEIS - MULHERES\n",
    "\n",
    "spcav_m = df_proporcoes.loc[df_proporcoes['regional'] == 'SP CAV', 'amostra_mulheres'].values[0]\n",
    "spint_m = df_proporcoes.loc[df_proporcoes['regional'] == 'SP Interior', 'amostra_mulheres'].values[0]\n",
    "sul_m = df_proporcoes.loc[df_proporcoes['regional'] == 'Sul', 'amostra_mulheres'].values[0]\n",
    "ne1_m = df_proporcoes.loc[df_proporcoes['regional'] == 'NE1', 'amostra_mulheres'].values[0]\n",
    "mg_m = df_proporcoes.loc[df_proporcoes['regional'] == 'MG', 'amostra_mulheres'].values[0]\n",
    "co_m = df_proporcoes.loc[df_proporcoes['regional'] == 'CO', 'amostra_mulheres'].values[0]\n",
    "rj_m = df_proporcoes.loc[df_proporcoes['regional'] == 'RJ', 'amostra_mulheres'].values[0]\n",
    "ne2_m = df_proporcoes.loc[df_proporcoes['regional'] == 'NE2', 'amostra_mulheres'].values[0]\n",
    "norte_m = df_proporcoes.loc[df_proporcoes['regional'] == 'Norte', 'amostra_mulheres'].values[0]\n",
    "es_m = df_proporcoes.loc[df_proporcoes['regional'] == 'ES', 'amostra_mulheres'].values[0]\n",
    "\n",
    "print(f'Amostra mulheres SP CAV: {spcav_m}')\n",
    "print(f'Amostra mulheres SP Int: {spint_m}')\n",
    "print(f'Amostra mulheres Sul: {sul_m}')\n",
    "print(f'Amostra mulheres NE1: {ne1_m}')\n",
    "print(f'Amostra mulheres MG: {mg_m}')\n",
    "print(f'Amostra mulheres CO: {co_m}')\n",
    "print(f'Amostra mulheres RJ: {rj_m}')\n",
    "print(f'Amostra mulheres NE2: {ne2_m}')\n",
    "print(f'Amostra mulheres Norte: {norte_m}')\n",
    "print(f'Amostra hommulheresens ES: {es_m}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho da amostra necessária: 270 para 90% de nível de confiança e 5% de margem de erro.\n"
     ]
    }
   ],
   "source": [
    "#COCHRAN FORMULA FOR SAMPLE SIZE - PACIENTE PARTICULAR\n",
    "\n",
    "def cochran_sample_size(confidence, estimated_proportion, margin_of_error):\n",
    "    z = np.abs(stats.norm.ppf((1 - confidence) / 2))  # Calculate Z-score\n",
    "    n = (z**2 * estimated_proportion * (1 - estimated_proportion)) / margin_of_error**2\n",
    "    return n\n",
    "\n",
    "# Example values\n",
    "confidence_level = 0.90  # 99% confidence\n",
    "estimated_proportion = 0.05  # Estimated proportion (50% for maximum sample size)\n",
    "margin_of_error = 0.05  # Margin of error (1%)\n",
    "\n",
    "# Calculate sample size\n",
    "sample_size_part = cochran_sample_size(confidence_level, estimated_proportion, margin_of_error)\n",
    "\n",
    "print(f\"Tamanho da amostra necessária: {int(sample_size_part)} para {confidence_level*100:.0f}% de nível de confiança e {margin_of_error*100:.0f}% de margem de erro.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regional</th>\n",
       "      <th>qtd_pacientes_homens</th>\n",
       "      <th>qtd_pacientes_mulheres</th>\n",
       "      <th>total_pacientes</th>\n",
       "      <th>proporcao_homens</th>\n",
       "      <th>proporcao_mulheres</th>\n",
       "      <th>tamanho_amostra</th>\n",
       "      <th>amostra_homens</th>\n",
       "      <th>amostra_mulheres</th>\n",
       "      <th>tamanho_amostra_part</th>\n",
       "      <th>amostra_homens_part</th>\n",
       "      <th>amostra_mulheres_part</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ES</td>\n",
       "      <td>107660</td>\n",
       "      <td>196503</td>\n",
       "      <td>7612762</td>\n",
       "      <td>0.014142</td>\n",
       "      <td>0.025812</td>\n",
       "      <td>16587.241503</td>\n",
       "      <td>235</td>\n",
       "      <td>429</td>\n",
       "      <td>3151.575885</td>\n",
       "      <td>45</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Norte</td>\n",
       "      <td>135914</td>\n",
       "      <td>250751</td>\n",
       "      <td>7612762</td>\n",
       "      <td>0.017853</td>\n",
       "      <td>0.032938</td>\n",
       "      <td>16587.241503</td>\n",
       "      <td>297</td>\n",
       "      <td>547</td>\n",
       "      <td>3151.575885</td>\n",
       "      <td>57</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NE2</td>\n",
       "      <td>180588</td>\n",
       "      <td>362902</td>\n",
       "      <td>7612762</td>\n",
       "      <td>0.023722</td>\n",
       "      <td>0.047670</td>\n",
       "      <td>16587.241503</td>\n",
       "      <td>394</td>\n",
       "      <td>791</td>\n",
       "      <td>3151.575885</td>\n",
       "      <td>75</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RJ</td>\n",
       "      <td>216168</td>\n",
       "      <td>404848</td>\n",
       "      <td>7612762</td>\n",
       "      <td>0.028395</td>\n",
       "      <td>0.053180</td>\n",
       "      <td>16587.241503</td>\n",
       "      <td>472</td>\n",
       "      <td>883</td>\n",
       "      <td>3151.575885</td>\n",
       "      <td>90</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CO</td>\n",
       "      <td>231395</td>\n",
       "      <td>428580</td>\n",
       "      <td>7612762</td>\n",
       "      <td>0.030396</td>\n",
       "      <td>0.056298</td>\n",
       "      <td>16587.241503</td>\n",
       "      <td>505</td>\n",
       "      <td>934</td>\n",
       "      <td>3151.575885</td>\n",
       "      <td>96</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MG</td>\n",
       "      <td>268464</td>\n",
       "      <td>485084</td>\n",
       "      <td>7612762</td>\n",
       "      <td>0.035265</td>\n",
       "      <td>0.063720</td>\n",
       "      <td>16587.241503</td>\n",
       "      <td>585</td>\n",
       "      <td>1057</td>\n",
       "      <td>3151.575885</td>\n",
       "      <td>112</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NE1</td>\n",
       "      <td>293109</td>\n",
       "      <td>564595</td>\n",
       "      <td>7612762</td>\n",
       "      <td>0.038502</td>\n",
       "      <td>0.074164</td>\n",
       "      <td>16587.241503</td>\n",
       "      <td>639</td>\n",
       "      <td>1231</td>\n",
       "      <td>3151.575885</td>\n",
       "      <td>122</td>\n",
       "      <td>234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sul</td>\n",
       "      <td>378680</td>\n",
       "      <td>669421</td>\n",
       "      <td>7612762</td>\n",
       "      <td>0.049743</td>\n",
       "      <td>0.087934</td>\n",
       "      <td>16587.241503</td>\n",
       "      <td>826</td>\n",
       "      <td>1459</td>\n",
       "      <td>3151.575885</td>\n",
       "      <td>157</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SP Interior</td>\n",
       "      <td>440010</td>\n",
       "      <td>748676</td>\n",
       "      <td>7612762</td>\n",
       "      <td>0.057799</td>\n",
       "      <td>0.098345</td>\n",
       "      <td>16587.241503</td>\n",
       "      <td>959</td>\n",
       "      <td>1632</td>\n",
       "      <td>3151.575885</td>\n",
       "      <td>183</td>\n",
       "      <td>310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SP CAV</td>\n",
       "      <td>438208</td>\n",
       "      <td>811206</td>\n",
       "      <td>7612762</td>\n",
       "      <td>0.057562</td>\n",
       "      <td>0.106559</td>\n",
       "      <td>16587.241503</td>\n",
       "      <td>955</td>\n",
       "      <td>1768</td>\n",
       "      <td>3151.575885</td>\n",
       "      <td>182</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      regional  qtd_pacientes_homens  qtd_pacientes_mulheres  total_pacientes  \\\n",
       "0           ES                107660                  196503          7612762   \n",
       "1        Norte                135914                  250751          7612762   \n",
       "2          NE2                180588                  362902          7612762   \n",
       "3           RJ                216168                  404848          7612762   \n",
       "4           CO                231395                  428580          7612762   \n",
       "5           MG                268464                  485084          7612762   \n",
       "6          NE1                293109                  564595          7612762   \n",
       "7          Sul                378680                  669421          7612762   \n",
       "8  SP Interior                440010                  748676          7612762   \n",
       "9       SP CAV                438208                  811206          7612762   \n",
       "\n",
       "   proporcao_homens  proporcao_mulheres  tamanho_amostra  amostra_homens  \\\n",
       "0          0.014142            0.025812     16587.241503             235   \n",
       "1          0.017853            0.032938     16587.241503             297   \n",
       "2          0.023722            0.047670     16587.241503             394   \n",
       "3          0.028395            0.053180     16587.241503             472   \n",
       "4          0.030396            0.056298     16587.241503             505   \n",
       "5          0.035265            0.063720     16587.241503             585   \n",
       "6          0.038502            0.074164     16587.241503             639   \n",
       "7          0.049743            0.087934     16587.241503             826   \n",
       "8          0.057799            0.098345     16587.241503             959   \n",
       "9          0.057562            0.106559     16587.241503             955   \n",
       "\n",
       "   amostra_mulheres  tamanho_amostra_part  amostra_homens_part  \\\n",
       "0               429           3151.575885                   45   \n",
       "1               547           3151.575885                   57   \n",
       "2               791           3151.575885                   75   \n",
       "3               883           3151.575885                   90   \n",
       "4               934           3151.575885                   96   \n",
       "5              1057           3151.575885                  112   \n",
       "6              1231           3151.575885                  122   \n",
       "7              1459           3151.575885                  157   \n",
       "8              1632           3151.575885                  183   \n",
       "9              1768           3151.575885                  182   \n",
       "\n",
       "   amostra_mulheres_part  \n",
       "0                     82  \n",
       "1                    104  \n",
       "2                    151  \n",
       "3                    168  \n",
       "4                    178  \n",
       "5                    201  \n",
       "6                    234  \n",
       "7                    278  \n",
       "8                    310  \n",
       "9                    336  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_proporcoes['tamanho_amostra_part'] = sample_size_part\n",
    "df_proporcoes['amostra_homens_part'] = (df_proporcoes['proporcao_homens'] * sample_size_part).apply(lambda x: math.ceil(x))\n",
    "df_proporcoes['amostra_mulheres_part'] = (df_proporcoes['proporcao_mulheres'] * sample_size_part).apply(lambda x: math.ceil(x))\n",
    "\n",
    "df_proporcoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amostra homens SP CAV: 182\n",
      "Amostra homens SP Int: 183\n",
      "Amostra homens Sul: 157\n",
      "Amostra homens NE1: 122\n",
      "Amostra homens MG: 112\n",
      "Amostra homens CO: 96\n",
      "Amostra homens RJ: 90\n",
      "Amostra homens NE2: 75\n",
      "Amostra homens Norte: 57\n",
      "Amostra homens ES: 45\n"
     ]
    }
   ],
   "source": [
    "#TRANSFORMANDO VALORES DA TABELA EM VARIÁVEIS - HOMENS\n",
    "\n",
    "spcav_h_part = df_proporcoes.loc[df_proporcoes['regional'] == 'SP CAV', 'amostra_homens_part'].values[0]\n",
    "spint_h_part = df_proporcoes.loc[df_proporcoes['regional'] == 'SP Interior', 'amostra_homens_part'].values[0]\n",
    "sul_h_part = df_proporcoes.loc[df_proporcoes['regional'] == 'Sul', 'amostra_homens_part'].values[0]\n",
    "ne1_h_part = df_proporcoes.loc[df_proporcoes['regional'] == 'NE1', 'amostra_homens_part'].values[0]\n",
    "mg_h_part = df_proporcoes.loc[df_proporcoes['regional'] == 'MG', 'amostra_homens_part'].values[0]\n",
    "co_h_part = df_proporcoes.loc[df_proporcoes['regional'] == 'CO', 'amostra_homens_part'].values[0]\n",
    "rj_h_part = df_proporcoes.loc[df_proporcoes['regional'] == 'RJ', 'amostra_homens_part'].values[0]\n",
    "ne2_h_part = df_proporcoes.loc[df_proporcoes['regional'] == 'NE2', 'amostra_homens_part'].values[0]\n",
    "norte_h_part = df_proporcoes.loc[df_proporcoes['regional'] == 'Norte', 'amostra_homens_part'].values[0]\n",
    "es_h_part = df_proporcoes.loc[df_proporcoes['regional'] == 'ES', 'amostra_homens_part'].values[0]\n",
    "\n",
    "print(f'Amostra homens SP CAV: {spcav_h_part}')\n",
    "print(f'Amostra homens SP Int: {spint_h_part}')\n",
    "print(f'Amostra homens Sul: {sul_h_part}')\n",
    "print(f'Amostra homens NE1: {ne1_h_part}')\n",
    "print(f'Amostra homens MG: {mg_h_part}')\n",
    "print(f'Amostra homens CO: {co_h_part}')\n",
    "print(f'Amostra homens RJ: {rj_h_part}')\n",
    "print(f'Amostra homens NE2: {ne2_h_part}')\n",
    "print(f'Amostra homens Norte: {norte_h_part}')\n",
    "print(f'Amostra homens ES: {es_h_part}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amostra mulheres SP CAV: 336\n",
      "Amostra mulheres SP Int: 310\n",
      "Amostra mulheres Sul: 278\n",
      "Amostra mulheres NE1: 234\n",
      "Amostra mulheres MG: 201\n",
      "Amostra mulheres CO: 178\n",
      "Amostra mulheres RJ: 168\n",
      "Amostra mulheres NE2: 151\n",
      "Amostra mulheres Norte: 104\n",
      "Amostra hommulheresens ES: 82\n"
     ]
    }
   ],
   "source": [
    "#TRANSFORMANDO VALORES DA TABELA EM VARIÁVEIS - MULHERES\n",
    "\n",
    "spcav_m_part = df_proporcoes.loc[df_proporcoes['regional'] == 'SP CAV', 'amostra_mulheres_part'].values[0]\n",
    "spint_m_part = df_proporcoes.loc[df_proporcoes['regional'] == 'SP Interior', 'amostra_mulheres_part'].values[0]\n",
    "sul_m_part = df_proporcoes.loc[df_proporcoes['regional'] == 'Sul', 'amostra_mulheres_part'].values[0]\n",
    "ne1_m_part = df_proporcoes.loc[df_proporcoes['regional'] == 'NE1', 'amostra_mulheres_part'].values[0]\n",
    "mg_m_part = df_proporcoes.loc[df_proporcoes['regional'] == 'MG', 'amostra_mulheres_part'].values[0]\n",
    "co_m_part = df_proporcoes.loc[df_proporcoes['regional'] == 'CO', 'amostra_mulheres_part'].values[0]\n",
    "rj_m_part = df_proporcoes.loc[df_proporcoes['regional'] == 'RJ', 'amostra_mulheres_part'].values[0]\n",
    "ne2_m_part = df_proporcoes.loc[df_proporcoes['regional'] == 'NE2', 'amostra_mulheres_part'].values[0]\n",
    "norte_m_part = df_proporcoes.loc[df_proporcoes['regional'] == 'Norte', 'amostra_mulheres_part'].values[0]\n",
    "es_m_part = df_proporcoes.loc[df_proporcoes['regional'] == 'ES', 'amostra_mulheres_part'].values[0]\n",
    "\n",
    "print(f'Amostra mulheres SP CAV: {spcav_m_part}')\n",
    "print(f'Amostra mulheres SP Int: {spint_m_part}')\n",
    "print(f'Amostra mulheres Sul: {sul_m_part}')\n",
    "print(f'Amostra mulheres NE1: {ne1_m_part}')\n",
    "print(f'Amostra mulheres MG: {mg_m_part}')\n",
    "print(f'Amostra mulheres CO: {co_m_part}')\n",
    "print(f'Amostra mulheres RJ: {rj_m_part}')\n",
    "print(f'Amostra mulheres NE2: {ne2_m_part}')\n",
    "print(f'Amostra mulheres Norte: {norte_m_part}')\n",
    "print(f'Amostra hommulheresens ES: {es_m_part}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_pacientes = f\"\"\"\n",
    "sql query\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MATERIALIZACAO DA AMOSTRA DE PACIENTES EM DF\n",
    "\n",
    "df_pac = execute_athena_query(query_pacientes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id_paciente', 'cpfpaciente', 'min_dt_pg', 'max_dt_pg',\n",
       "       'anos_utilizacao', 'meses_utilizacao', 'dias_ult_utilizacao',\n",
       "       'meses_ult_utilizacao', 'estado', 'cidade', 'idade', 'sexo',\n",
       "       'endereco_paciente', 'qtd_consultas', 'qtd_exam_proc', 'qtd_particular',\n",
       "       'tt_consulta', 'tm_consulta', 'tt_exam_proc', 'qtd_orc_nao_executado',\n",
       "       'tt_orc_nao_executado', 'tm_utilizacao', 'pac_particular',\n",
       "       'ultimo_canal', 'qtd_canais', 'mudou_canal', 'regional',\n",
       "       'nm_especialidade', 'tabela', 'unidade', 'endereco_unidade',\n",
       "       'cidade_clinica', 'score_r', 'score_f', 'score_v'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pac.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRATATIVAS NA BASE DE PACIENTES\n",
    "\n",
    "# REMOVER ACENTOS DAS CIDADES \n",
    "def remove_accents(text):\n",
    "    return ''.join([c for c in unicodedata.normalize('NFD', text) if not unicodedata.combining(c)])\n",
    "\n",
    "# Apply the function to your columns\n",
    "df_pac['cidade'] = df_pac['cidade'].apply(remove_accents)\n",
    "df_pac['cidade_clinica'] = df_pac['cidade_clinica'].apply(remove_accents)\n",
    "\n",
    "# APLICAR REGRA DE MESMA CIDADE\n",
    "df_pac['mesma_cidade'] = (df_pac['cidade'] == df_pac['cidade_clinica']).astype(int)\n",
    "\n",
    "# APLICAR FAIXAS DE IDADE E UTILIZACAO\n",
    "age_bins = [0, 19, 24, 29, 34, 39, 44, 49, 54, 59, float('inf')]\n",
    "age_labels = ['A: 0-18', 'B: 19-23', 'C: 24-28', 'D: 29-33', 'E: 34-38', 'F: 39-43', 'G: 44-48', 'H: 49-53', 'I: 54-58', 'J: 59+']\n",
    "\n",
    "# Add a new column 'faixa_etaria' based on 'idade' column\n",
    "df_pac['faixa_etaria'] = pd.cut(df_pac['idade'], bins=age_bins, labels=age_labels, right=False)\n",
    "\n",
    "# Define the age group bins and labels based on your classification\n",
    "time_bins = [0, 7, 13, 25, 37, 49, 61, float('inf')]\n",
    "time_labels = ['A: 0-6 meses', 'B: 7-12 meses', 'C: 1 ano', 'D: 2 anos', 'E: 3 anos', 'F: 4 anos', 'G: 5+ anos']\n",
    "\n",
    "# Add a new column 'faixa_etaria' based on 'idade' column\n",
    "df_pac['faixa_utilizacao'] = pd.cut(df_pac['meses_utilizacao'], bins=time_bins, labels=time_labels, right=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para traduzir as condições da regra em texto\n",
    "def translate_recency_score(dias_ult_utilizacao):\n",
    "    if dias_ult_utilizacao <= 45:\n",
    "        return '5. <= 45 dias'\n",
    "    elif 46 <= dias_ult_utilizacao <= 90:\n",
    "        return '4. 46-90 dias'\n",
    "    elif 91 <= dias_ult_utilizacao <= 180:\n",
    "        return '3. 91-180 dias'\n",
    "    elif 181 <= dias_ult_utilizacao <= 365:\n",
    "        return '2. 181-365 dias'\n",
    "    elif dias_ult_utilizacao > 365:\n",
    "        return '1. > 365 dias'\n",
    "    else:\n",
    "        return 'N/A'\n",
    "\n",
    "# Aplicar a função à coluna 'dias_ult_utilizacao' e criar a nova coluna 'score_r_text'\n",
    "df_pac['score_recencia'] = df_pac['dias_ult_utilizacao'].apply(translate_recency_score)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Função para traduzir as condições da regra em texto\n",
    "def translate_frequency_score(qtd_consultas):\n",
    "    if qtd_consultas == 1:\n",
    "        return '1. 1 consulta'\n",
    "    elif qtd_consultas == 2:\n",
    "        return '2. 2 consultas'\n",
    "    elif qtd_consultas == 3:\n",
    "        return '3. 3 consultas'\n",
    "    elif 4 <= qtd_consultas <= 5:\n",
    "        return '4. 4-5 consultas'\n",
    "    elif qtd_consultas > 5:\n",
    "        return '5. > 5 consultas'\n",
    "    else:\n",
    "        return 'N/A'\n",
    "\n",
    "# Aplicar a função à coluna 'qtd_consultas' e 'tm_utilizacao' e criar a nova coluna 'score_f_text'\n",
    "df_pac['score_frequencia'] = df_pac['qtd_consultas'].map(lambda x: translate_frequency_score(x))\n",
    "\n",
    "# Função para traduzir as condições da regra em texto\n",
    "def translate_value_score(tm_utilizacao):\n",
    "    if tm_utilizacao is None:\n",
    "        return 'N/A'\n",
    "    elif tm_utilizacao <= 28:\n",
    "        return '1. <= R$ 28'\n",
    "    elif 28.01 <= tm_utilizacao <= 56:\n",
    "        return '2. R$ 28.01 - R$ 56'\n",
    "    elif 56.01 <= tm_utilizacao <= 84:\n",
    "        return '3. R$ 56.01 - R$ 84'\n",
    "    elif 84.01 <= tm_utilizacao <= 140:\n",
    "        return '4. R$ 84.01 - R$ 140'\n",
    "    elif tm_utilizacao > 140:\n",
    "        return '5. > R$ 140'\n",
    "    else:\n",
    "        return 'N/A'\n",
    "\n",
    "# Aplicar a função à coluna 'tm_utilizacao' e criar a nova coluna 'score_v_text'\n",
    "df_pac['score_valor'] = df_pac['tm_utilizacao'].map(lambda x: translate_value_score(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row 468 due to 'NOT_FOUND' error.\n",
      "Skipping row 1933 due to 'NOT_FOUND' error.\n",
      "Skipping row 1971 due to 'NOT_FOUND' error.\n",
      "Skipping row 2094 due to 'NOT_FOUND' error.\n",
      "Skipping row 2290 due to 'NOT_FOUND' error.\n",
      "Skipping row 2325 due to 'NOT_FOUND' error.\n",
      "Skipping row 2326 due to 'NOT_FOUND' error.\n",
      "Skipping row 4572 due to 'NOT_FOUND' error.\n",
      "Skipping row 4783 due to 'NOT_FOUND' error.\n",
      "Skipping row 4853 due to 'NOT_FOUND' error.\n",
      "Skipping row 4867 due to 'NOT_FOUND' error.\n",
      "Skipping row 5460 due to 'NOT_FOUND' error.\n",
      "Skipping row 5682 due to 'NOT_FOUND' error.\n",
      "Skipping row 6553 due to 'NOT_FOUND' error.\n",
      "Skipping row 6554 due to 'NOT_FOUND' error.\n",
      "Skipping row 6671 due to 'NOT_FOUND' error.\n",
      "Skipping row 8734 due to 'NOT_FOUND' error.\n",
      "Skipping row 10166 due to 'NOT_FOUND' error.\n",
      "Skipping row 10663 due to 'NOT_FOUND' error.\n",
      "Skipping row 10755 due to 'NOT_FOUND' error.\n",
      "Skipping row 10927 due to 'NOT_FOUND' error.\n",
      "Skipping row 11147 due to 'NOT_FOUND' error.\n",
      "Skipping row 11352 due to 'NOT_FOUND' error.\n",
      "Skipping row 11497 due to 'NOT_FOUND' error.\n",
      "Skipping row 11982 due to 'NOT_FOUND' error.\n",
      "Skipping row 12067 due to 'NOT_FOUND' error.\n",
      "Skipping row 12583 due to 'NOT_FOUND' error.\n",
      "Skipping row 12959 due to 'NOT_FOUND' error.\n",
      "Skipping row 14414 due to 'NOT_FOUND' error.\n",
      "Skipping row 15612 due to 'NOT_FOUND' error.\n",
      "Skipping row 15722 due to 'NOT_FOUND' error.\n",
      "Skipping row 15744 due to 'NOT_FOUND' error.\n",
      "Skipping row 15779 due to 'NOT_FOUND' error.\n",
      "Skipping row 15897 due to 'NOT_FOUND' error.\n",
      "Skipping row 15965 due to 'NOT_FOUND' error.\n",
      "Skipping row 16094 due to 'NOT_FOUND' error.\n",
      "Skipping row 16361 due to 'NOT_FOUND' error.\n",
      "Skipping row 16441 due to 'NOT_FOUND' error.\n",
      "Skipping row 16460 due to 'NOT_FOUND' error.\n",
      "Skipping row 16543 due to 'NOT_FOUND' error.\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the DataFrame\n",
    "for index, row in df_pac.iterrows():\n",
    "    source = row['endereco_paciente']\n",
    "    destination = row['endereco_unidade']\n",
    "\n",
    "    try:\n",
    "        if source and destination:\n",
    "            direction_result = gmaps_client.directions(source, destination)\n",
    "\n",
    "            # Extract the distance and duration values\n",
    "            if direction_result:\n",
    "                legs = direction_result[0]['legs'][0]\n",
    "                distance_text = legs['distance']['text']\n",
    "                distance_value = legs['distance']['value']\n",
    "                duration_text = legs['duration']['text']\n",
    "                duration_value = legs['duration']['value']\n",
    "\n",
    "                # You can then use these values as needed, for example, store them in your DataFrame\n",
    "                df_pac.at[index, 'distance_text'] = distance_text\n",
    "                df_pac.at[index, 'distance_value'] = distance_value\n",
    "                df_pac.at[index, 'duration_text'] = duration_text\n",
    "                df_pac.at[index, 'duration_value'] = duration_value\n",
    "\n",
    "    except ApiError as e:\n",
    "        if \"NOT_FOUND\" in str(e):\n",
    "            # Handle the \"NOT_FOUND\" error (e.g., skip to the next row)\n",
    "            print(f\"Skipping row {index} due to 'NOT_FOUND' error.\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOSTRAR DF\n",
    "\n",
    "df_pac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MATERIALIZAR A BASE FINAL EM EXCEL\n",
    "\n",
    "df_pac.to_excel('bd_paciente_geo.xlsx', index=False, sheet_name='base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_titular = f\"\"\"\n",
    "sql query\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MATERIALIZACAO DA AMOSTRA DE TITULAR EM DF\n",
    "\n",
    "df_tit = execute_athena_query(query_titular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRATATIVAS NA BASE DE TITULAR\n",
    "\n",
    "# REMOVER ACENTOS DAS CIDADES \n",
    "def remove_accents(text):\n",
    "    return ''.join([c for c in unicodedata.normalize('NFD', text) if not unicodedata.combining(c)])\n",
    "\n",
    "# Apply the function to your columns\n",
    "df_tit['cidade'] = df_tit['cidade'].apply(remove_accents)\n",
    "df_tit['cidade_clinica'] = df_tit['cidade_clinica'].apply(remove_accents)\n",
    "\n",
    "# APLICAR REGRA DE MESMA CIDADE\n",
    "df_tit['mesma_cidade'] = (df_tit['cidade'] == df_tit['cidade_clinica']).astype(int)\n",
    "\n",
    "# APLICAR FAIXAS DE IDADE E UTILIZACAO\n",
    "age_bins = [0, 19, 24, 29, 34, 39, 44, 49, 54, 59, float('inf')]\n",
    "age_labels = ['A: 0-18', 'B: 19-23', 'C: 24-28', 'D: 29-33', 'E: 34-38', 'F: 39-43', 'G: 44-48', 'H: 49-53', 'I: 54-58', 'J: 59+']\n",
    "\n",
    "# Add a new column 'faixa_etaria' based on 'idade' column\n",
    "df_tit['faixa_etaria'] = pd.cut(df_tit['idade'], bins=age_bins, labels=age_labels, right=False)\n",
    "\n",
    "# Define the age group bins and labels based on your classification\n",
    "time_bins = [0, 7, 13, 25, 37, 49, 61, float('inf')]\n",
    "time_labels = ['A: 0-6 meses', 'B: 7-12 meses', 'C: 1 ano', 'D: 2 anos', 'E: 3 anos', 'F: 4 anos', 'G: 5+ anos']\n",
    "\n",
    "# Add a new column 'faixa_etaria' based on 'idade' column\n",
    "df_tit['faixa_utilizacao'] = pd.cut(df_tit['meses_utilizacao'], bins=time_bins, labels=time_labels, right=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para traduzir as condições da regra em texto\n",
    "def translate_recency_score(dias_ult_utilizacao):\n",
    "    if dias_ult_utilizacao <= 45:\n",
    "        return '5. <= 45 dias'\n",
    "    elif 46 <= dias_ult_utilizacao <= 90:\n",
    "        return '4. 46-90 dias'\n",
    "    elif 91 <= dias_ult_utilizacao <= 180:\n",
    "        return '3. 91-180 dias'\n",
    "    elif 181 <= dias_ult_utilizacao <= 365:\n",
    "        return '2. 181-365 dias'\n",
    "    elif dias_ult_utilizacao > 365:\n",
    "        return '1. > 365 dias'\n",
    "    else:\n",
    "        return 'N/A'\n",
    "\n",
    "# Aplicar a função à coluna 'dias_ult_utilizacao' e criar a nova coluna 'score_r_text'\n",
    "df_tit['score_recencia'] = df_tit['dias_ult_utilizacao'].apply(translate_recency_score)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Função para traduzir as condições da regra em texto\n",
    "def translate_frequency_score(qtd_consultas):\n",
    "    if qtd_consultas == 1:\n",
    "        return '1. 1 consulta'\n",
    "    elif qtd_consultas == 2:\n",
    "        return '2. 2 consultas'\n",
    "    elif qtd_consultas == 3:\n",
    "        return '3. 3 consultas'\n",
    "    elif 4 <= qtd_consultas <= 5:\n",
    "        return '4. 4-5 consultas'\n",
    "    elif qtd_consultas > 5:\n",
    "        return '5. > 5 consultas'\n",
    "    else:\n",
    "        return 'N/A'\n",
    "\n",
    "# Aplicar a função à coluna 'qtd_consultas' e 'tm_utilizacao' e criar a nova coluna 'score_f_text'\n",
    "df_tit['score_frequencia'] = df_tit['qtd_consultas'].map(lambda x: translate_frequency_score(x))\n",
    "\n",
    "# Função para traduzir as condições da regra em texto\n",
    "def translate_value_score(tm_utilizacao):\n",
    "    if tm_utilizacao is None:\n",
    "        return 'N/A'\n",
    "    elif tm_utilizacao <= 28:\n",
    "        return '1. <= R$ 28'\n",
    "    elif 28.01 <= tm_utilizacao <= 56:\n",
    "        return '2. R$ 28.01 - R$ 56'\n",
    "    elif 56.01 <= tm_utilizacao <= 84:\n",
    "        return '3. R$ 56.01 - R$ 84'\n",
    "    elif 84.01 <= tm_utilizacao <= 140:\n",
    "        return '4. R$ 84.01 - R$ 140'\n",
    "    elif tm_utilizacao > 140:\n",
    "        return '5. > R$ 140'\n",
    "    else:\n",
    "        return 'N/A'\n",
    "\n",
    "# Aplicar a função à coluna 'tm_utilizacao' e criar a nova coluna 'score_v_text'\n",
    "df_tit['score_valor'] = df_tit['tm_utilizacao'].map(lambda x: translate_value_score(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row 27 due to 'NOT_FOUND' error at 2023-10-13 17:52:10.\n",
      "Skipping row 818 due to 'NOT_FOUND' error at 2023-10-13 18:00:00.\n",
      "Skipping row 1943 due to 'NOT_FOUND' error at 2023-10-13 18:10:56.\n",
      "Skipping row 2667 due to 'NOT_FOUND' error at 2023-10-13 18:17:27.\n",
      "Skipping row 2674 due to 'NOT_FOUND' error at 2023-10-13 18:17:31.\n",
      "Skipping row 2675 due to 'NOT_FOUND' error at 2023-10-13 18:17:32.\n",
      "Skipping row 2770 due to 'NOT_FOUND' error at 2023-10-13 18:18:27.\n",
      "Skipping row 2886 due to 'NOT_FOUND' error at 2023-10-13 18:19:36.\n",
      "Skipping row 3162 due to 'NOT_FOUND' error at 2023-10-13 18:22:16.\n",
      "Skipping row 3455 due to 'NOT_FOUND' error at 2023-10-13 18:25:04.\n",
      "Skipping row 3636 due to 'NOT_FOUND' error at 2023-10-13 18:26:47.\n",
      "Skipping row 4099 due to 'NOT_FOUND' error at 2023-10-13 18:30:56.\n",
      "Skipping row 4223 due to 'NOT_FOUND' error at 2023-10-13 18:31:55.\n",
      "Skipping row 4829 due to 'NOT_FOUND' error at 2023-10-13 18:37:37.\n",
      "Skipping row 5521 due to 'NOT_FOUND' error at 2023-10-13 18:43:35.\n",
      "Skipping row 5785 due to 'NOT_FOUND' error at 2023-10-13 18:45:49.\n",
      "Skipping row 8744 due to 'NOT_FOUND' error at 2023-10-13 19:13:41.\n",
      "Skipping row 8802 due to 'NOT_FOUND' error at 2023-10-13 19:14:12.\n",
      "Skipping row 10752 due to 'NOT_FOUND' error at 2023-10-13 19:32:05.\n",
      "Skipping row 10814 due to 'NOT_FOUND' error at 2023-10-13 19:32:40.\n",
      "Skipping row 10859 due to 'NOT_FOUND' error at 2023-10-13 19:33:05.\n",
      "Skipping row 11561 due to 'NOT_FOUND' error at 2023-10-13 19:39:25.\n",
      "Skipping row 11572 due to 'NOT_FOUND' error at 2023-10-13 19:39:32.\n",
      "Skipping row 11640 due to 'NOT_FOUND' error at 2023-10-13 19:40:09.\n",
      "Skipping row 11755 due to 'NOT_FOUND' error at 2023-10-13 19:41:04.\n",
      "Skipping row 11811 due to 'NOT_FOUND' error at 2023-10-13 19:41:35.\n",
      "Skipping row 12032 due to 'NOT_FOUND' error at 2023-10-13 19:43:33.\n",
      "Skipping row 12294 due to 'NOT_FOUND' error at 2023-10-13 19:45:58.\n",
      "Skipping row 13231 due to 'NOT_FOUND' error at 2023-10-13 19:54:45.\n",
      "Skipping row 13556 due to 'NOT_FOUND' error at 2023-10-13 19:57:45.\n",
      "Skipping row 14968 due to 'NOT_FOUND' error at 2023-10-13 20:11:04.\n",
      "Skipping row 15367 due to 'NOT_FOUND' error at 2023-10-13 20:14:56.\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the DataFrame\n",
    "for index, row in df_tit.iterrows():\n",
    "    source = row['endereco_paciente']\n",
    "    destination = row['endereco_unidade']\n",
    "\n",
    "    try:\n",
    "        if source and destination:\n",
    "            direction_result = gmaps_client.directions(source, destination)\n",
    "\n",
    "            # Extract the distance and duration values\n",
    "            if direction_result:\n",
    "                legs = direction_result[0]['legs'][0]\n",
    "                distance_text = legs['distance']['text']\n",
    "                distance_value = legs['distance']['value']\n",
    "                duration_text = legs['duration']['text']\n",
    "                duration_value = legs['duration']['value']\n",
    "\n",
    "                # You can then use these values as needed, for example, store them in your DataFrame\n",
    "                df_tit.at[index, 'distance_text'] = distance_text\n",
    "                df_tit.at[index, 'distance_value'] = distance_value\n",
    "                df_tit.at[index, 'duration_text'] = duration_text\n",
    "                df_tit.at[index, 'duration_value'] = duration_value\n",
    "\n",
    "    except ApiError as e:\n",
    "        if \"NOT_FOUND\" in str(e):\n",
    "            current_time = time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            # Handle the \"NOT_FOUND\" error (e.g., skip to the next row)\n",
    "            print(f\"Skipping row {index} due to 'NOT_FOUND' error at {current_time}.\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOSTRAR DF\n",
    "\n",
    "df_tit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MATERIALIZAR A BASE FINAL EM EXCEL\n",
    "\n",
    "df_tit.to_excel('bd_titular_geo.xlsx', index=False, sheet_name='base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_dependente = f\"\"\"\n",
    "s\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MATERIALIZACAO DA AMOSTRA DE DEPENDENTES EM DF\n",
    "\n",
    "df_dep = execute_athena_query(query_dependente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRATATIVAS NA BASE DE PACIENTES\n",
    "\n",
    "# REMOVER ACENTOS DAS CIDADES\n",
    "def remove_accents(text):\n",
    "    return ''.join([c for c in unicodedata.normalize('NFD', text) if not unicodedata.combining(c)])\n",
    "\n",
    "# Apply the function to your columns\n",
    "df_dep['cidade'] = df_dep['cidade'].apply(remove_accents)\n",
    "df_dep['cidade_clinica'] = df_dep['cidade_clinica'].apply(remove_accents)\n",
    "\n",
    "# APLICAR REGRA DE MESMA CIDADE\n",
    "df_dep['mesma_cidade'] = (df_dep['cidade'] == df_dep['cidade_clinica']).astype(int)\n",
    "\n",
    "# APLICAR FAIXAS DE IDADE E UTILIZACAO\n",
    "age_bins = [0, 19, 24, 29, 34, 39, 44, 49, 54, 59, float('inf')]\n",
    "age_labels = ['A: 0-18', 'B: 19-23', 'C: 24-28', 'D: 29-33', 'E: 34-38', 'F: 39-43', 'G: 44-48', 'H: 49-53', 'I: 54-58', 'J: 59+']\n",
    "\n",
    "# Add a new column 'faixa_etaria' based on 'idade' column\n",
    "df_dep['faixa_etaria'] = pd.cut(df_dep['idade'], bins=age_bins, labels=age_labels, right=False)\n",
    "\n",
    "# Define the age group bins and labels based on your classification\n",
    "time_bins = [0, 7, 13, 25, 37, 49, 61, float('inf')]\n",
    "time_labels = ['A: 0-6 meses', 'B: 7-12 meses', 'C: 1 ano', 'D: 2 anos', 'E: 3 anos', 'F: 4 anos', 'G: 5+ anos']\n",
    "\n",
    "# Add a new column 'faixa_etaria' based on 'idade' column\n",
    "df_dep['faixa_utilizacao'] = pd.cut(df_dep['meses_utilizacao'], bins=time_bins, labels=time_labels, right=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para traduzir as condições da regra em texto\n",
    "def translate_recency_score(dias_ult_utilizacao):\n",
    "    if dias_ult_utilizacao <= 45:\n",
    "        return '5. <= 45 dias'\n",
    "    elif 46 <= dias_ult_utilizacao <= 90:\n",
    "        return '4. 46-90 dias'\n",
    "    elif 91 <= dias_ult_utilizacao <= 180:\n",
    "        return '3. 91-180 dias'\n",
    "    elif 181 <= dias_ult_utilizacao <= 365:\n",
    "        return '2. 181-365 dias'\n",
    "    elif dias_ult_utilizacao > 365:\n",
    "        return '1. > 365 dias'\n",
    "    else:\n",
    "        return 'N/A'\n",
    "\n",
    "# Aplicar a função à coluna 'dias_ult_utilizacao' e criar a nova coluna 'score_r_text'\n",
    "df_dep['score_recencia'] = df_dep['dias_ult_utilizacao'].apply(translate_recency_score)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Função para traduzir as condições da regra em texto\n",
    "def translate_frequency_score(qtd_consultas):\n",
    "    if qtd_consultas == 1:\n",
    "        return '1. 1 consulta'\n",
    "    elif qtd_consultas == 2:\n",
    "        return '2. 2 consultas'\n",
    "    elif qtd_consultas == 3:\n",
    "        return '3. 3 consultas'\n",
    "    elif 4 <= qtd_consultas <= 5:\n",
    "        return '4. 4-5 consultas'\n",
    "    elif qtd_consultas > 5:\n",
    "        return '5. > 5 consultas'\n",
    "    else:\n",
    "        return 'N/A'\n",
    "\n",
    "# Aplicar a função à coluna 'qtd_consultas' e 'tm_utilizacao' e criar a nova coluna 'score_f_text'\n",
    "df_dep['score_frequencia'] = df_dep['qtd_consultas'].map(lambda x: translate_frequency_score(x))\n",
    "\n",
    "# Função para traduzir as condições da regra em texto\n",
    "def translate_value_score(tm_utilizacao):\n",
    "    if tm_utilizacao is None:\n",
    "        return 'N/A'\n",
    "    elif tm_utilizacao <= 28:\n",
    "        return '1. <= R$ 28'\n",
    "    elif 28.01 <= tm_utilizacao <= 56:\n",
    "        return '2. R$ 28.01 - R$ 56'\n",
    "    elif 56.01 <= tm_utilizacao <= 84:\n",
    "        return '3. R$ 56.01 - R$ 84'\n",
    "    elif 84.01 <= tm_utilizacao <= 140:\n",
    "        return '4. R$ 84.01 - R$ 140'\n",
    "    elif tm_utilizacao > 140:\n",
    "        return '5. > R$ 140'\n",
    "    else:\n",
    "        return 'N/A'\n",
    "\n",
    "# Aplicar a função à coluna 'tm_utilizacao' e criar a nova coluna 'score_valor'\n",
    "df_dep['score_valor'] = df_dep['tm_utilizacao'].map(lambda x: translate_value_score(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row 132 due to 'NOT_FOUND' error at 2023-10-14 06:59:23.\n",
      "Skipping row 230 due to 'NOT_FOUND' error at 2023-10-14 06:59:51.\n",
      "Skipping row 392 due to 'NOT_FOUND' error at 2023-10-14 07:00:37.\n",
      "Skipping row 1215 due to 'NOT_FOUND' error at 2023-10-14 07:04:29.\n",
      "Skipping row 2438 due to 'NOT_FOUND' error at 2023-10-14 07:10:21.\n",
      "Skipping row 2540 due to 'NOT_FOUND' error at 2023-10-14 07:10:51.\n",
      "Skipping row 3989 due to 'NOT_FOUND' error at 2023-10-14 07:19:23.\n",
      "Skipping row 4745 due to 'NOT_FOUND' error at 2023-10-14 07:24:29.\n",
      "Skipping row 5844 due to 'NOT_FOUND' error at 2023-10-14 07:31:49.\n",
      "Skipping row 7283 due to 'NOT_FOUND' error at 2023-10-14 07:41:39.\n",
      "Skipping row 7381 due to 'NOT_FOUND' error at 2023-10-14 07:42:16.\n",
      "Skipping row 7384 due to 'NOT_FOUND' error at 2023-10-14 07:42:17.\n",
      "Skipping row 7636 due to 'NOT_FOUND' error at 2023-10-14 07:43:49.\n",
      "Skipping row 8780 due to 'NOT_FOUND' error at 2023-10-14 07:51:21.\n",
      "Skipping row 9646 due to 'NOT_FOUND' error at 2023-10-14 07:57:02.\n",
      "Skipping row 10009 due to 'NOT_FOUND' error at 2023-10-14 07:59:22.\n",
      "Skipping row 10086 due to 'NOT_FOUND' error at 2023-10-14 07:59:50.\n",
      "Skipping row 10089 due to 'NOT_FOUND' error at 2023-10-14 07:59:51.\n",
      "Skipping row 10289 due to 'NOT_FOUND' error at 2023-10-14 08:01:03.\n",
      "Skipping row 10412 due to 'NOT_FOUND' error at 2023-10-14 08:01:46.\n",
      "Skipping row 11428 due to 'NOT_FOUND' error at 2023-10-14 08:08:09.\n",
      "Skipping row 12487 due to 'NOT_FOUND' error at 2023-10-14 08:15:19.\n",
      "Skipping row 12611 due to 'NOT_FOUND' error at 2023-10-14 08:16:09.\n",
      "Skipping row 12902 due to 'NOT_FOUND' error at 2023-10-14 08:18:05.\n",
      "Skipping row 13101 due to 'NOT_FOUND' error at 2023-10-14 08:19:26.\n",
      "Skipping row 13142 due to 'NOT_FOUND' error at 2023-10-14 08:19:42.\n",
      "Skipping row 13319 due to 'NOT_FOUND' error at 2023-10-14 08:20:54.\n",
      "Skipping row 13629 due to 'NOT_FOUND' error at 2023-10-14 08:22:58.\n",
      "Skipping row 13650 due to 'NOT_FOUND' error at 2023-10-14 08:23:06.\n",
      "Skipping row 13720 due to 'NOT_FOUND' error at 2023-10-14 08:23:33.\n",
      "Skipping row 13812 due to 'NOT_FOUND' error at 2023-10-14 08:24:10.\n",
      "Skipping row 13825 due to 'NOT_FOUND' error at 2023-10-14 08:24:15.\n",
      "Skipping row 13874 due to 'NOT_FOUND' error at 2023-10-14 08:24:34.\n",
      "Skipping row 13926 due to 'NOT_FOUND' error at 2023-10-14 08:24:54.\n",
      "Skipping row 14032 due to 'NOT_FOUND' error at 2023-10-14 08:25:36.\n",
      "Skipping row 15589 due to 'NOT_FOUND' error at 2023-10-14 08:35:58.\n",
      "Skipping row 15995 due to 'NOT_FOUND' error at 2023-10-14 08:38:42.\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the DataFrame\n",
    "for index, row in df_dep.iterrows():\n",
    "    source = row['endereco_paciente']\n",
    "    destination = row['endereco_unidade']\n",
    "\n",
    "    try:\n",
    "        if source and destination:\n",
    "            direction_result = gmaps_client.directions(source, destination)\n",
    "\n",
    "            # Extract the distance and duration values\n",
    "            if direction_result:\n",
    "                legs = direction_result[0]['legs'][0]\n",
    "                distance_text = legs['distance']['text']\n",
    "                distance_value = legs['distance']['value']\n",
    "                duration_text = legs['duration']['text']\n",
    "                duration_value = legs['duration']['value']\n",
    "\n",
    "                # You can then use these values as needed, for example, store them in your DataFrame\n",
    "                df_dep.at[index, 'distance_text'] = distance_text\n",
    "                df_dep.at[index, 'distance_value'] = distance_value\n",
    "                df_dep.at[index, 'duration_text'] = duration_text\n",
    "                df_dep.at[index, 'duration_value'] = duration_value\n",
    "\n",
    "    except ApiError as e:\n",
    "        if \"NOT_FOUND\" in str(e):\n",
    "            current_time = time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            # Handle the \"NOT_FOUND\" error (e.g., skip to the next row)\n",
    "            print(f\"Skipping row {index} due to 'NOT_FOUND' error at {current_time}.\")\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOSTRAR DF\n",
    "\n",
    "df_dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MATERIALIZAR A BASE FINAL EM EXCEL\n",
    "\n",
    "df_dep.to_excel('bd_dependente_geo.xlsx', index=False, sheet_name='base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_particular = f\"\"\"\n",
    "sql query\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MATERIALIZACAO DA AMOSTRA DE PARTICULAR EM DF\n",
    "\n",
    "df_part = execute_athena_query(query_particular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRATATIVAS NA BASE DE PACIENTES\n",
    "\n",
    "# REMOVER ACENTOS DAS CIDADES\n",
    "def remove_accents(text):\n",
    "    return ''.join([c for c in unicodedata.normalize('NFD', text) if not unicodedata.combining(c)])\n",
    "\n",
    "# Apply the function to your columns\n",
    "df_part['cidade'] = df_part['cidade'].apply(remove_accents)\n",
    "df_part['cidade_clinica'] = df_part['cidade_clinica'].apply(remove_accents)\n",
    "\n",
    "# APLICAR REGRA DE MESMA CIDADE\n",
    "df_part['mesma_cidade'] = (df_part['cidade'] == df_part['cidade_clinica']).astype(int)\n",
    "\n",
    "# APLICAR FAIXAS DE IDADE E UTILIZACAO\n",
    "age_bins = [0, 19, 24, 29, 34, 39, 44, 49, 54, 59, float('inf')]\n",
    "age_labels = ['A: 0-18', 'B: 19-23', 'C: 24-28', 'D: 29-33', 'E: 34-38', 'F: 39-43', 'G: 44-48', 'H: 49-53', 'I: 54-58', 'J: 59+']\n",
    "\n",
    "# Add a new column 'faixa_etaria' based on 'idade' column\n",
    "df_part['faixa_etaria'] = pd.cut(df_part['idade'], bins=age_bins, labels=age_labels, right=False)\n",
    "\n",
    "# Define the age group bins and labels based on your classification\n",
    "time_bins = [0, 7, 13, 25, 37, 49, 61, float('inf')]\n",
    "time_labels = ['A: 0-6 meses', 'B: 7-12 meses', 'C: 1 ano', 'D: 2 anos', 'E: 3 anos', 'F: 4 anos', 'G: 5+ anos']\n",
    "\n",
    "# Add a new column 'faixa_etaria' based on 'idade' column\n",
    "df_part['faixa_utilizacao'] = pd.cut(df_part['meses_utilizacao'], bins=time_bins, labels=time_labels, right=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para traduzir as condições da regra em texto\n",
    "def translate_recency_score(dias_ult_utilizacao):\n",
    "    if dias_ult_utilizacao <= 45:\n",
    "        return '5. <= 45 dias'\n",
    "    elif 46 <= dias_ult_utilizacao <= 90:\n",
    "        return '4. 46-90 dias'\n",
    "    elif 91 <= dias_ult_utilizacao <= 180:\n",
    "        return '3. 91-180 dias'\n",
    "    elif 181 <= dias_ult_utilizacao <= 365:\n",
    "        return '2. 181-365 dias'\n",
    "    elif dias_ult_utilizacao > 365:\n",
    "        return '1. > 365 dias'\n",
    "    else:\n",
    "        return 'N/A'\n",
    "\n",
    "# Aplicar a função à coluna 'dias_ult_utilizacao' e criar a nova coluna 'score_r_text'\n",
    "df_part['score_recencia'] = df_part['dias_ult_utilizacao'].apply(translate_recency_score)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Função para traduzir as condições da regra em texto\n",
    "def translate_frequency_score(qtd_consultas):\n",
    "    if qtd_consultas == 1:\n",
    "        return '1. 1 consulta'\n",
    "    elif qtd_consultas == 2:\n",
    "        return '2. 2 consultas'\n",
    "    elif qtd_consultas == 3:\n",
    "        return '3. 3 consultas'\n",
    "    elif 4 <= qtd_consultas <= 5:\n",
    "        return '4. 4-5 consultas'\n",
    "    elif qtd_consultas > 5:\n",
    "        return '5. > 5 consultas'\n",
    "    else:\n",
    "        return 'N/A'\n",
    "\n",
    "# Aplicar a função à coluna 'qtd_consultas' e 'tm_utilizacao' e criar a nova coluna 'score_f_text'\n",
    "df_part['score_frequencia'] = df_part['qtd_consultas'].map(lambda x: translate_frequency_score(x))\n",
    "\n",
    "# Função para traduzir as condições da regra em texto\n",
    "def translate_value_score(tm_utilizacao):\n",
    "    if tm_utilizacao is None:\n",
    "        return 'N/A'\n",
    "    elif tm_utilizacao <= 28:\n",
    "        return '1. <= R$ 28'\n",
    "    elif 28.01 <= tm_utilizacao <= 56:\n",
    "        return '2. R$ 28.01 - R$ 56'\n",
    "    elif 56.01 <= tm_utilizacao <= 84:\n",
    "        return '3. R$ 56.01 - R$ 84'\n",
    "    elif 84.01 <= tm_utilizacao <= 140:\n",
    "        return '4. R$ 84.01 - R$ 140'\n",
    "    elif tm_utilizacao > 140:\n",
    "        return '5. > R$ 140'\n",
    "    else:\n",
    "        return 'N/A'\n",
    "\n",
    "# Aplicar a função à coluna 'tm_utilizacao' e criar a nova coluna 'score_valor'\n",
    "df_part['score_valor'] = df_part['tm_utilizacao'].map(lambda x: translate_value_score(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row 318 due to 'NOT_FOUND' error at 2023-10-14 08:52:01.\n",
      "Skipping row 1008 due to 'NOT_FOUND' error at 2023-10-14 08:56:23.\n",
      "Skipping row 1155 due to 'NOT_FOUND' error at 2023-10-14 08:57:20.\n",
      "Skipping row 1526 due to 'NOT_FOUND' error at 2023-10-14 08:59:39.\n",
      "Skipping row 1815 due to 'NOT_FOUND' error at 2023-10-14 09:01:31.\n",
      "Skipping row 2982 due to 'NOT_FOUND' error at 2023-10-14 09:08:49.\n",
      "Skipping row 3005 due to 'NOT_FOUND' error at 2023-10-14 09:08:57.\n",
      "Skipping row 3010 due to 'NOT_FOUND' error at 2023-10-14 09:08:59.\n",
      "Skipping row 3094 due to 'NOT_FOUND' error at 2023-10-14 09:09:32.\n",
      "Skipping row 3128 due to 'NOT_FOUND' error at 2023-10-14 09:09:46.\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the DataFrame\n",
    "for index, row in df_part.iterrows():\n",
    "    source = row['endereco_paciente']\n",
    "    destination = row['endereco_unidade']\n",
    "\n",
    "    try:\n",
    "        if source and destination:\n",
    "            direction_result = gmaps_client.directions(source, destination)\n",
    "\n",
    "            # Extract the distance and duration values\n",
    "            if direction_result:\n",
    "                legs = direction_result[0]['legs'][0]\n",
    "                distance_text = legs['distance']['text']\n",
    "                distance_value = legs['distance']['value']\n",
    "                duration_text = legs['duration']['text']\n",
    "                duration_value = legs['duration']['value']\n",
    "\n",
    "                # You can then use these values as needed, for example, store them in your DataFrame\n",
    "                df_part.at[index, 'distance_text'] = distance_text\n",
    "                df_part.at[index, 'distance_value'] = distance_value\n",
    "                df_part.at[index, 'duration_text'] = duration_text\n",
    "                df_part.at[index, 'duration_value'] = duration_value\n",
    "\n",
    "    except ApiError as e:\n",
    "        if \"NOT_FOUND\" in str(e):\n",
    "            current_time = time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            # Handle the \"NOT_FOUND\" error (e.g., skip to the next row)\n",
    "            print(f\"Skipping row {index} due to 'NOT_FOUND' error at {current_time}.\")\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOSTRAR DF\n",
    "\n",
    "df_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MATERIALIZAR A BASE FINAL EM EXCEL\n",
    "\n",
    "df_part.to_excel('bd_particular_geo.xlsx', index=False, sheet_name='base')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
